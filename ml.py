# -*- coding: utf-8 -*-
"""ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EFSdnymRW7yTR-fSfejNSxaO7O78Zxm_
"""

from google.colab import drive

drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.datasets import load_breast_cancer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import label_binarize as lb
from sklearn.metrics import precision_recall_curve
from collections import Counter
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

!pip install light-the-torch >> /.tmp
!ltt install torch torchvision >> /.tmp
!pip install fastai --upgrade >> /.tmp

df = pd.read_csv('/content/drive/MyDrive/ml-dataset/WSN-DS.csv')

df

df.head()

df.tail()

df.shape

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df.shape

df.info()

df.describe()

class_counts = df["label"].value_counts()
print(class_counts)

plt.figure(figsize=(8, 6))
plt.pie(class_counts, labels=class_counts.index, autopct="%1.1f%%")
plt.title("Pie Chart of Class Distribution")
plt.show()

X = df.drop("label", axis=1)
y = df["label"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

y_train.value_counts()

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier # Import the DecisionTreeClassifier class

print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

# Define base classifier (replace with your choice)
base_model =  KNeighborsClassifier()  # Example

# Define ensemble model with Bagging
ensemble_model = BaggingClassifier(base_estimator=base_model, n_estimators=100)

# Train the ensemble model
ensemble_model.fit(X_train, y_train)

base_model =  DecisionTreeClassifier()  # Example

# Define ensemble model with Bagging
ensemble_model = BaggingClassifier(base_estimator=base_model, n_estimators=100)

# Train the ensemble model
ensemble_model.fit(X_train, y_train)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.datasets import load_breast_cancer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import label_binarize as lb
from sklearn.metrics import precision_recall_curve
from collections import Counter
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.naive_bayes import GaussianNB
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier # Import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Train standalone classifiers (replace with your choices)
naive_bayes = GaussianNB()
decision_tree = DecisionTreeClassifier()

knn = KNeighborsClassifier()
random_forest = RandomForestClassifier()

naive_bayes.fit(X_train, y_train)
decision_tree.fit(X_train, y_train)

knn.fit(X_train, y_train)
random_forest.fit(X_train, y_train)

# Make predictions on test data (assuming you have X_test)
ensemble_model_predictions = ensemble_model.predict(X_test) # Change from best_ensemble_model to ensemble_model
naive_bayes_predictions = naive_bayes.predict(X_test)
decision_tree_predictions = decision_tree.predict(X_test)

knn_predictions = knn.predict(X_test)
random_forest_predictions = random_forest.predict(X_test)

# Evaluate performance using metrics
ensemble_accuracy = accuracy_score(y_test, ensemble_model_predictions)
# ... similarly calculate accuracy for other models

print("Ensemble Model Accuracy:", ensemble_accuracy)
print("Naive Bayes Accuracy:", accuracy_score(y_test, naive_bayes_predictions))
print("Decision Tree Accuracy:", accuracy_score(y_test, decision_tree_predictions))

print("KNN Accuracy:", accuracy_score(y_test, knn_predictions))
print("Random Forest Accuracy:", accuracy_score(y_test, random_forest_predictions))

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(sampling_strategy=1) # Numerical value
# rus = RandomUnderSampler(sampling_strategy="not minority") # String
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

ax = y_train_rus.value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Under-sampling")

sns.boxplot(
    x = "",  # Set species as the categorical variable on the x-axis
    y = "sepal_length",  # Set sepal length as the numerical variable on the y-axis
    showmeans=True,  # Display the mean as a point within the box
    data=df
)

# Customize the plot (optional)
plt.xlabel('Species')
plt.ylabel('Sepal Length (cm)')
plt.title('Sepal Length Distribution by Species')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

species_counts.plot(kind='hist', color=['skyblue', 'lightgreen', 'pink'])  # Set colors for each bar
plt.xlabel('Species')
plt.ylabel('Count')
plt.title('Species Distribution (Histogram)')
plt.xticks(rotation=0)  # Rotate x-axis labels for readability
plt.tight_layout()
plt.show()

sns.pairplot(df, hue="species")  # Color points by species
plt.show()

sns.heatmap(correlation, annot=True)  # Display correlation values
plt.title('Correlation Matrix')
plt.show()

df

x = df.drop('species', axis=1)
y = df['species']

x

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=41)

x_train

X_test

y_train

y_test

df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

Logist = LogisticRegression(max_iter=10)

Logist.fit(x_train,y_train)

Logist.score(x_train, y_train)

y_predict = Logist.predict(x_test)

accuracy = accuracy_score(y_test, y_predict)
accuracy

report = classification_report(y_test, y_predict)

print(report)

y_pred_proba = Logist.predict_proba(x_test)

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_predict)
confusion_matrix

roc_auc_score(y_test,y_pred_proba,multi_class="ovr")



knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(x_train, y_train)

knn.score(x_train, y_train)

y_predict = knn.predict(x_test)

y_pred_proba = knn.predict_proba(x_test)[:, 1]

y_pred_proba = knn.predict_proba(x_test)

accuracy = accuracy_score(y_test, y_predict)
accuracy

report = classification_report(y_test, y_predict)

print(report)

y_predict

y_pred = knn.predict(x_test)
report = classification_report(y_test, y_pred)
print(report)

y_pred_prob = knn.predict_proba(x_test)

y_pred_prob

y_predict

roc_auc_score(y_test,y_pred_proba,multi_class="ovr")

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_predict)
confusion_matrix

from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error

clf = DecisionTreeClassifier()

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

roc_auc_score(y_test,y_pred_proba,multi_class="ovr")

from sklearn.metrics import confusion_matrix

confusion_matrix = confusion_matrix(y_test, y_pred)

confusion_matrix